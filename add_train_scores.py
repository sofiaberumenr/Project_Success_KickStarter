import json

with open('kickstarter_analysis.ipynb', 'r') as f:
    nb = json.load(f)

# Find the Model Comparison cell
comparison_idx = None
for i, cell in enumerate(nb['cells']):
    if cell['cell_type'] == 'code' and 'Compare all models' in ''.join(cell['source']):
        comparison_idx = i
        break

if comparison_idx:
    # Replace the comparison cell with updated version
    new_cell = {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "# Calculate train predictions for all models\n",
            "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
            "y_train_pred_dt = dt_model.predict(X_train)\n",
            "y_train_pred_rf = rf_model.predict(X_train)\n",
            "y_train_pred_gb = gb_model.predict(X_train)\n",
            "y_train_pred_knn = knn_model.predict(X_train_scaled)\n",
            "y_train_pred_svm = svm_model.predict(X_train_scaled)\n",
            "y_train_pred_nb = nb_model.predict(X_train_scaled)\n",
            "\n",
            "# Compare all models with train and test scores\n",
            "results = pd.DataFrame({\n",
            "    'Model': ['Logistic Regression', 'Decision Tree', 'Random Forest', \n",
            "              'Gradient Boosting', 'KNN', 'SVM', 'Naive Bayes'],\n",
            "    'Train Accuracy': [\n",
            "        accuracy_score(y_train, y_train_pred_lr),\n",
            "        accuracy_score(y_train, y_train_pred_dt),\n",
            "        accuracy_score(y_train, y_train_pred_rf),\n",
            "        accuracy_score(y_train, y_train_pred_gb),\n",
            "        accuracy_score(y_train, y_train_pred_knn),\n",
            "        accuracy_score(y_train, y_train_pred_svm),\n",
            "        accuracy_score(y_train, y_train_pred_nb)\n",
            "    ],\n",
            "    'Test Accuracy': [\n",
            "        accuracy_score(y_test, y_pred_lr),\n",
            "        accuracy_score(y_test, y_pred_dt),\n",
            "        accuracy_score(y_test, y_pred_rf),\n",
            "        accuracy_score(y_test, y_pred_gb),\n",
            "        accuracy_score(y_test, y_pred_knn),\n",
            "        accuracy_score(y_test, y_pred_svm),\n",
            "        accuracy_score(y_test, y_pred_nb)\n",
            "    ],\n",
            "    'Train F1': [\n",
            "        f1_score(y_train, y_train_pred_lr),\n",
            "        f1_score(y_train, y_train_pred_dt),\n",
            "        f1_score(y_train, y_train_pred_rf),\n",
            "        f1_score(y_train, y_train_pred_gb),\n",
            "        f1_score(y_train, y_train_pred_knn),\n",
            "        f1_score(y_train, y_train_pred_svm),\n",
            "        f1_score(y_train, y_train_pred_nb)\n",
            "    ],\n",
            "    'Test F1': [\n",
            "        f1_score(y_test, y_pred_lr),\n",
            "        f1_score(y_test, y_pred_dt),\n",
            "        f1_score(y_test, y_pred_rf),\n",
            "        f1_score(y_test, y_pred_gb),\n",
            "        f1_score(y_test, y_pred_knn),\n",
            "        f1_score(y_test, y_pred_svm),\n",
            "        f1_score(y_test, y_pred_nb)\n",
            "    ],\n",
            "    'Test ROC-AUC': [\n",
            "        roc_auc_score(y_test, y_pred_proba_lr),\n",
            "        roc_auc_score(y_test, y_pred_proba_dt),\n",
            "        roc_auc_score(y_test, y_pred_proba_rf),\n",
            "        roc_auc_score(y_test, y_pred_proba_gb),\n",
            "        roc_auc_score(y_test, y_pred_proba_knn),\n",
            "        roc_auc_score(y_test, y_pred_proba_svm),\n",
            "        roc_auc_score(y_test, y_pred_proba_nb)\n",
            "    ]\n",
            "})\n",
            "\n",
            "# Calculate overfitting (difference between train and test)\n",
            "results['Accuracy Gap'] = results['Train Accuracy'] - results['Test Accuracy']\n",
            "results['F1 Gap'] = results['Train F1'] - results['Test F1']\n",
            "\n",
            "print(\"\\n=== Model Comparison ===\")\n",
            "print(results.sort_values('Test ROC-AUC', ascending=False))\n",
            "\n",
            "# Visualize train vs test performance\n",
            "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
            "\n",
            "# Accuracy comparison\n",
            "x = np.arange(len(results['Model']))\n",
            "width = 0.35\n",
            "axes[0, 0].bar(x - width/2, results['Train Accuracy'], width, label='Train', alpha=0.8)\n",
            "axes[0, 0].bar(x + width/2, results['Test Accuracy'], width, label='Test', alpha=0.8)\n",
            "axes[0, 0].set_xlabel('Model')\n",
            "axes[0, 0].set_ylabel('Accuracy')\n",
            "axes[0, 0].set_title('Train vs Test Accuracy')\n",
            "axes[0, 0].set_xticks(x)\n",
            "axes[0, 0].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
            "axes[0, 0].legend()\n",
            "axes[0, 0].grid(axis='y', alpha=0.3)\n",
            "\n",
            "# F1-Score comparison\n",
            "axes[0, 1].bar(x - width/2, results['Train F1'], width, label='Train', alpha=0.8)\n",
            "axes[0, 1].bar(x + width/2, results['Test F1'], width, label='Test', alpha=0.8)\n",
            "axes[0, 1].set_xlabel('Model')\n",
            "axes[0, 1].set_ylabel('F1-Score')\n",
            "axes[0, 1].set_title('Train vs Test F1-Score')\n",
            "axes[0, 1].set_xticks(x)\n",
            "axes[0, 1].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
            "axes[0, 1].legend()\n",
            "axes[0, 1].grid(axis='y', alpha=0.3)\n",
            "\n",
            "# Overfitting analysis - Accuracy Gap\n",
            "colors = ['red' if gap > 0.05 else 'green' for gap in results['Accuracy Gap']]\n",
            "axes[1, 0].bar(x, results['Accuracy Gap'], color=colors, alpha=0.7)\n",
            "axes[1, 0].set_xlabel('Model')\n",
            "axes[1, 0].set_ylabel('Accuracy Gap (Train - Test)')\n",
            "axes[1, 0].set_title('Overfitting Analysis (Accuracy)')\n",
            "axes[1, 0].set_xticks(x)\n",
            "axes[1, 0].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
            "axes[1, 0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
            "axes[1, 0].axhline(y=0.05, color='orange', linestyle='--', linewidth=0.5, label='5% threshold')\n",
            "axes[1, 0].legend()\n",
            "axes[1, 0].grid(axis='y', alpha=0.3)\n",
            "\n",
            "# ROC-AUC\n",
            "axes[1, 1].bar(x, results['Test ROC-AUC'], color='coral', alpha=0.7)\n",
            "axes[1, 1].set_xlabel('Model')\n",
            "axes[1, 1].set_ylabel('ROC-AUC')\n",
            "axes[1, 1].set_title('Test ROC-AUC Scores')\n",
            "axes[1, 1].set_xticks(x)\n",
            "axes[1, 1].set_xticklabels(results['Model'], rotation=45, ha='right')\n",
            "axes[1, 1].axhline(y=0.5, color='red', linestyle='--', linewidth=0.5, label='Random baseline')\n",
            "axes[1, 1].legend()\n",
            "axes[1, 1].grid(axis='y', alpha=0.3)\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.show()\n",
            "\n",
            "# Print overfitting summary\n",
            "print(\"\\n=== Overfitting Analysis ===\")\n",
            "print(\"Models with significant overfitting (>5% accuracy gap):\")\n",
            "overfitted = results[results['Accuracy Gap'] > 0.05]\n",
            "if len(overfitted) > 0:\n",
            "    print(overfitted[['Model', 'Train Accuracy', 'Test Accuracy', 'Accuracy Gap']])\n",
            "else:\n",
            "    print(\"No models show significant overfitting.\")"
        ]
    }
    
    nb['cells'][comparison_idx] = new_cell
    
    with open('kickstarter_analysis.ipynb', 'w') as f:
        json.dump(nb, f, indent=1)
    
    print("Updated comparison cell with train/test scores")
else:
    print("Could not find comparison cell")
